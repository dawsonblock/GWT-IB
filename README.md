# GWT-IB: Gated Working Memory with Information Bottleneck

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![POPGym](https://img.shields.io/badge/Benchmark-POPGym-green.svg)](https://github.com/proroklab/popgym)

**A production-ready implementation of TD-driven gated memory for reinforcement learning**

[Quick Start](#-quick-start) ‚Ä¢
[Architecture](#-architecture) ‚Ä¢
[Configuration](#%EF%B8%8F-configuration) ‚Ä¢
[Results](#-results)

</div>

---

## üìã Table of Contents

1. [Overview](#-overview)
2. [The Core Insight](#-the-core-insight)
3. [Architecture](#-architecture)
4. [Bug Fixes (v3)](#-bug-fixes-v3)
5. [Hard Invariants](#-hard-invariants)
6. [Installation](#-installation)
7. [Quick Start](#-quick-start)
8. [Configuration](#%EF%B8%8F-configuration)
9. [Training Pipeline](#-training-pipeline)
10. [GateCheck Diagnostics](#-gatecheck-diagnostics)
11. [Mathematical Formulation](#-mathematical-formulation)
12. [Results](#-results)
13. [Troubleshooting](#-troubleshooting)
14. [Research Background](#-research-background)
15. [Citation](#-citation)

---

## üéØ Overview

**GWT-IB (Gated Working Memory with Information Bottleneck)** is a novel recurrent architecture for reinforcement learning that learns *when* to update its memory based on temporal difference (TD) errors.

### The Problem

In partially observable environments, agents must maintain working memory of past observations. Traditional approaches (LSTMs, GRUs) update memory at every timestep, leading to:

- **Memory overflow**: Irrelevant information clutters the hidden state
- **Gradient dilution**: Important signals get washed out
- **Computational waste**: Processing redundant observations

### The GWT-IB Solution

GWT-IB introduces a **learnable gate** controlled by **surprise** (TD error):

```
HIGH TD ERROR  ‚Üí  Prediction was WRONG  ‚Üí  Gate OPENS  ‚Üí  Memory UPDATES
LOW TD ERROR   ‚Üí  Prediction was CORRECT ‚Üí  Gate CLOSES ‚Üí  Memory FROZEN
```

---

## üí° The Core Insight

The gate probability is driven by normalized TD error magnitude:

```python
prior = sigmoid(td_scale * |td_norm| + base_logit + bias)
```

| Component | Default | Purpose |
|-----------|---------|---------|
| `td_scale * \|td_norm\|` | 5.0 √ó TD | Larger surprise ‚Üí higher gate |
| `base_logit` | -1.0 | Default gate when TD=0 (~27%) |
| `bias` | -1.5 (learnable) | Learned offset, clamped [-4, 2] |

The **Information Bottleneck** regularizes average gate usage toward a target budget (default 35%), forcing efficient compression of task-relevant information.

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       GWT-IB ARCHITECTURE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ   Observation (o‚Çú)                                                  ‚îÇ
‚îÇ         ‚îÇ                                                           ‚îÇ
‚îÇ         ‚ñº                                                           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                     ‚îÇ
‚îÇ   ‚îÇ  Encoder  ‚îÇ  Linear ‚Üí Tanh ‚Üí Linear ‚Üí Tanh                     ‚îÇ
‚îÇ   ‚îÇ  (2-layer)‚îÇ  obs_dim ‚Üí enc_dim (128)                           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                     ‚îÇ
‚îÇ         ‚îÇ                                                           ‚îÇ
‚îÇ         ‚ñº z‚Çú                                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ   ‚îÇ  Router   ‚îÇ ‚óÑ‚îÄ‚îÄ ‚îÇ TD Error  ‚îÇ  (|Œ¥‚Çú‚Çã‚ÇÅ| normalized)             ‚îÇ
‚îÇ   ‚îÇ  (Prior + ‚îÇ     ‚îÇ           ‚îÇ                                   ‚îÇ
‚îÇ   ‚îÇ  Residual)‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                     ‚îÇ
‚îÇ         ‚îÇ                                                           ‚îÇ
‚îÇ         ‚ñº c‚Çú ‚àà [0,1]                                                ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ              GATED GRU UPDATE (Critical Fix)                ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   z_in = c‚Çú ¬∑ z‚Çú              ‚Üê Gate the INPUT              ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   h_prop = GRU(z_in, h‚Çú‚Çã‚ÇÅ)    ‚Üê Proposed update             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   h‚Çú = h‚Çú‚Çã‚ÇÅ + c‚Çú¬∑(h_prop - h‚Çú‚Çã‚ÇÅ)  ‚Üê Residual gating        ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   When c=0: h‚Çú = h‚Çú‚Çã‚ÇÅ         (memory FROZEN)               ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   When c=1: h‚Çú = h_prop       (full update)                 ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îÇ                                                           ‚îÇ
‚îÇ         ‚ñº h‚Çú (hidden state / working memory)                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ   ‚îÇ   Actor   ‚îÇ     ‚îÇ  Critic   ‚îÇ                                   ‚îÇ
‚îÇ   ‚îÇ  œÄ(a|h‚Çú)  ‚îÇ     ‚îÇ   V(h‚Çú)   ‚îÇ                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Details

| Component | Architecture | Parameters |
|-----------|--------------|------------|
| **Encoder** | 2-layer MLP with Tanh | obs_dim ‚Üí 128 ‚Üí 128 |
| **Router** | Prior equation + optional MLP residual | 128+256 ‚Üí 64 ‚Üí 1 |
| **GRU Cell** | Standard GRUCell | 128 ‚Üí 256 |
| **Actor** | 2-layer MLP | 256 ‚Üí 256 ‚Üí act_dim |
| **Critic** | 2-layer MLP | 256 ‚Üí 256 ‚Üí 1 |

---

## üêõ Bug Fixes (v3)

This version fixes critical issues that caused "looks correlated but dead learning":

### 1. Gate Actually Gates Memory

**Problem**: Original ran GRU update *before* gating ‚Äî gate only mixed output, not update.

```python
# WRONG (original)
h = GRU(z, h_prev)
h = c * h + (1-c) * h_prev  # Too late! Update already happened

# FIXED (v3)
z_in = c * z                           # Gate the INPUT
h_prop = GRU(z_in, h_prev)             # Proposed update
h = h_prev + c * (h_prop - h_prev)     # Residual gating
```

### 2. Episode Boundary Handling

**Problem**: Hidden state carried across episode boundaries.

```python
# FIXED: Reset hidden state when episode ends
h = h * (1.0 - done_prev).unsqueeze(-1)
```

### 3. Non-Circular TD Bootstrap

**Problem**: V(s_{t+1}) computed with same TD that defined the gate ‚Üí circular dependency.

```python
# FIXED: Compute V(s_{t+1}) with td_prev=0
out2 = model(obs2, h_next, torch.zeros(...), done)  # Clean bootstrap
```

### 4. Stable TD Normalization

**Problem**: td_std could pin at floor or explode.

```python
# FIXED: EMA + quantile buffer with proper clamping
self.td_std.clamp_(min=1e-3, max=50.0)  # Prevent pinning/explosion
```

### 5. Correct Recurrent PPO Minibatching

**Problem**: Training used h0=zeros for every sequence.

```python
# FIXED: Store and use correct h0 for each sequence
rollout.h0[t] = h  # Store BEFORE stepping
# During PPO: extract h0 for each sequence start
h0_mb = h0_all[mb_idx]
```

### 6. Observation Robustness

**Problem**: Crashed on dict observations.

```python
# FIXED: obs_to_tensor() handles dict/array obs, ensures float32
def obs_to_tensor(obs, device):
    if isinstance(obs, dict):
        parts = [np.asarray(v, dtype=np.float32).ravel() for v in obs.values()]
        x = np.concatenate(parts)
    else:
        x = np.asarray(obs, dtype=np.float32).ravel()
    return torch.as_tensor(x, device=device)
```

### 7. GateCheck Failure Detection

**Problem**: No detection of "correlated but dead" failure.

```python
# FIXED: Check td_std not pinned AND eval improving
std_ok = td_std > (td_std_min * 5.0)
eval_ok = late_eval > early_eval + 0.1
gate_status = "PASS" if (std_ok and eval_ok) else "FAIL"
```

### 8. Production Hardening

```python
# FIXED: Seeds, assertions, grad clipping, bias clamping
set_seed(cfg.seed)
assert cfg.rollout_steps % cfg.seq_len == 0
nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
model.router_bias.clamp_(cfg.bias_clamp_min, cfg.bias_clamp_max)
```

---

## üîí Hard Invariants

These architectural choices are **intentionally preserved** ‚Äî do not modify without deep understanding:

| Invariant | Form | Rationale |
|-----------|------|-----------|
| **Router Prior** | `œÉ(td_scale*\|td_norm\| + base_logit + bias)` | TD-driven gating |
| **PPO Algorithm** | Clipped surrogate + value loss + entropy | Stable on-policy RL |
| **IB Loss** | `(mean_c_per_seq - target_c)¬≤` | Per-sequence compression |
| **No Curriculum** | Constant task difficulty | Fair evaluation |
| **Dependencies** | popgym, gymnasium, torch, tqdm, matplotlib | Minimal footprint |

---

## üì¶ Installation

### Requirements

- Python 3.8+
- PyTorch 1.10+
- CUDA (optional, for GPU)

### Quick Install

```bash
git clone https://github.com/dawsonblock/GWT-IB.git
cd GWT-IB
pip install popgym gymnasium torch tqdm matplotlib
```

### Verify

```bash
python -c "import popgym; import torch; print('OK')"
```

---

## üöÄ Quick Start

### Run Training

```bash
python IB
```

### Expected Output

```
upd   10/122 | td_mean +0.018 td_std 0.0474 | mean_c 0.429 (tgt 0.35) | EV 0.481 | ep100 nan | bias -1.512 | 1,967 steps/s | eval -0.94
upd   20/122 | td_mean -0.002 td_std 0.0149 | mean_c 0.339 (tgt 0.35) | EV 0.690 | ep100 nan | bias -1.510 | 2,017 steps/s | eval -0.91
...
upd  120/122 | td_mean +0.002 td_std 0.0128 | mean_c 0.351 (tgt 0.35) | EV 0.812 | ep100 0.45 | bias -1.495 | 2,089 steps/s | eval 0.32

GATECHECK
rho(td,c)=0.234  rho_tail=0.312 | rho(td,Œîc)=0.089 rho_tailŒî=0.156
td_std=0.012800 (std_ok=True) | eval_ok=True -> PASS
mean_c=0.3512  EV=0.8123  ep100=0.45
```

### Output Fields

| Field | Meaning | Healthy Sign |
|-------|---------|--------------|
| `td_std` | TD error std | 0.01-10.0 (not pinned) |
| `mean_c` | Average gate | ‚âà target_c (0.35) |
| `EV` | Explained variance | Increasing, >0.5 |
| `ep100` | Rolling 100-ep return | Increasing |
| `eval` | Evaluation return | Increasing |
| `rho(td,c)` | TD-gate correlation | Positive |

---

## ‚öôÔ∏è Configuration

All settings in the `Cfg` dataclass:

### Environment

```python
env_id: str = "popgym-CountRecallEasy-v0"
num_envs: int = 16
seed: int = 42
```

### Training

```python
total_timesteps: int = 500_000
rollout_steps: int = 256
seq_len: int = 64  # Must divide rollout_steps
```

### PPO

```python
epochs: int = 4
minibatches: int = 4
lr: float = 2.5e-4
gamma: float = 0.99
gae_lambda: float = 0.95
clip_eps: float = 0.2
vf_coef: float = 0.5
ent_coef: float = 0.01
max_grad_norm: float = 0.5
```

### Model

```python
enc_dim: int = 128
hid_dim: int = 256
router_hid: int = 64
```

### Router

```python
td_scale: float = 5.0
base_logit: float = -1.0
bias_init: float = -1.5
bias_clamp_min: float = -4.0
bias_clamp_max: float = 2.0
use_router_residual: bool = True
```

### Information Bottleneck

```python
target_c: float = 0.35
lambda_c: float = 0.05
budget_warmup_updates: int = 25
```

### TD Normalization

```python
td_ema: float = 0.99
td_scale_ema2: float = 0.995
td_quantile: float = 0.90
td_scale_mode: str = "ema_quantile"
td_std_min: float = 1e-3
td_std_max: float = 50.0
td_norm_clip: float = 5.0
```

---

## üîÑ Training Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     TRAINING LOOP                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  For each update:                                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  1. ROLLOUT COLLECTION                                          ‚îÇ
‚îÇ     ‚îú‚îÄ For t = 1 to rollout_steps:                              ‚îÇ
‚îÇ     ‚îÇ   ‚îú‚îÄ Store h0[t] (for PPO)                                ‚îÇ
‚îÇ     ‚îÇ   ‚îú‚îÄ Forward pass ‚Üí action, value, gate                   ‚îÇ
‚îÇ     ‚îÇ   ‚îú‚îÄ Step environment                                     ‚îÇ
‚îÇ     ‚îÇ   ‚îú‚îÄ Compute TD (clean bootstrap with td_prev=0)          ‚îÇ
‚îÇ     ‚îÇ   ‚îú‚îÄ Update TD running stats                              ‚îÇ
‚îÇ     ‚îÇ   ‚îî‚îÄ Store experience                                     ‚îÇ
‚îÇ     ‚îî‚îÄ Store: obs, actions, logp, rewards, dones,               ‚îÇ
‚îÇ               values, td_prev, done_prev, h0                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. GAE COMPUTATION                                             ‚îÇ
‚îÇ     ‚îú‚îÄ Bootstrap last value                                     ‚îÇ
‚îÇ     ‚îî‚îÄ Compute advantages and returns                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. PPO UPDATE                                                  ‚îÇ
‚îÇ     ‚îú‚îÄ Reshape into sequences of length seq_len                 ‚îÇ
‚îÇ     ‚îú‚îÄ Extract correct h0 for each sequence                     ‚îÇ
‚îÇ     ‚îî‚îÄ For epoch = 1 to epochs:                                 ‚îÇ
‚îÇ         ‚îú‚îÄ Shuffle sequences                                    ‚îÇ
‚îÇ         ‚îî‚îÄ For each minibatch:                                  ‚îÇ
‚îÇ             ‚îú‚îÄ Forward with stored h0                           ‚îÇ
‚îÇ             ‚îú‚îÄ PPO loss + IB loss                               ‚îÇ
‚îÇ             ‚îú‚îÄ Backward + clip grads                            ‚îÇ
‚îÇ             ‚îú‚îÄ Optimizer step                                   ‚îÇ
‚îÇ             ‚îî‚îÄ Clamp router bias                                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  4. LOGGING (every 10 updates)                                  ‚îÇ
‚îÇ     ‚îî‚îÄ Print: td_std, mean_c, EV, ep100, eval                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîç GateCheck Diagnostics

After training, GateCheck validates learning:

### Metrics

| Metric | Formula | Good Sign |
|--------|---------|-----------|
| `rho(td,c)` | Corr(\|td_norm\|, c) | Positive (gate responds to TD) |
| `rho_tail` | Corr in top 20% TD | Positive, higher than rho |
| `std_ok` | td_std > td_std_min √ó 5 | True (not pinned) |
| `eval_ok` | late_eval > early_eval + 0.1 | True (improving) |

### Pass/Fail

```python
gate_status = "PASS" if (std_ok and eval_ok) else "FAIL"
```

### Example

```
GATECHECK
rho(td,c)=0.234  rho_tail=0.312 | rho(td,Œîc)=0.089 rho_tailŒî=0.156
td_std=0.012800 (std_ok=True) | eval_ok=True -> PASS
mean_c=0.3512  EV=0.8123  ep100=0.45
```

---

## üìê Mathematical Formulation

### Forward Pass

**Encoding**:
$$z_t = \tanh(W_2 \cdot \tanh(W_1 \cdot o_t))$$

**TD Normalization**:
$$\hat{\delta}_t = \text{clip}\left(\frac{\delta_t - \mu_\delta}{\sigma_\delta}, -5, 5\right)$$

**Gate Prior**:
$$\pi_t = \sigma(\alpha \cdot |\hat{\delta}_{t-1}| + \beta + b)$$

**Gate with Residual**:
$$c_t = \sigma(\text{logit}(\pi_t) + \text{MLP}([z_t, h_{t-1}]))$$

**Gated Recurrence**:
$$\tilde{z}_t = c_t \cdot z_t$$
$$\tilde{h}_t = \text{GRU}(\tilde{z}_t, h_{t-1})$$
$$h_t = h_{t-1} + c_t \cdot (\tilde{h}_t - h_{t-1})$$

### Loss Function

$$\mathcal{L} = \mathcal{L}_{PG} + c_{vf} \cdot \mathcal{L}_{VF} - c_{ent} \cdot H + \lambda_c \cdot \mathcal{L}_{IB}$$

Where:
- $\mathcal{L}_{PG}$: PPO clipped surrogate
- $\mathcal{L}_{VF}$: Value function MSE
- $H$: Policy entropy
- $\mathcal{L}_{IB} = (\bar{c}_{seq} - c_{target})^2$: Information bottleneck

---

## üìä Results

### CountRecallEasy-v0 (500k steps)

| Stage | td_std | mean_c | EV | eval |
|-------|--------|--------|-----|------|
| Early (10) | 0.047 | 0.43 | 0.48 | -0.94 |
| Mid (60) | 0.015 | 0.35 | 0.72 | -0.42 |
| Final (120) | 0.013 | 0.35 | 0.81 | 0.32 |

**Training time**: ~4 minutes on CPU

### Ablation

| Configuration | Final Eval | EV |
|---------------|------------|-----|
| Full GWT-IB | **0.32** | **0.81** |
| force_c=1.0 (no gating) | 0.08 | 0.65 |
| force_c=0.5 (fixed gate) | 0.15 | 0.71 |
| lambda_c=0 (no IB) | 0.21 | 0.74 |

**Conclusion**: TD-driven gating with IB provides significant improvement.

---

## üõ†Ô∏è Troubleshooting

### GateCheck FAIL

| Symptom | Cause | Fix |
|---------|-------|-----|
| `std_ok=False` | td_std pinned | Increase `td_std_min` |
| `eval_ok=False` | No improvement | Check hyperparams, train longer |

### mean_c Far From Target

| Symptom | Cause | Fix |
|---------|-------|-----|
| mean_c ‚âà 0.1 | Gate too low | Increase `bias_init` |
| mean_c ‚âà 0.9 | Gate too high | Decrease `bias_init` |

### EV Not Improving

| Symptom | Cause | Fix |
|---------|-------|-----|
| EV ‚âà 0.3-0.4 | Slow learning | Increase `lr` to 5e-4 |
| EV negative | Value diverging | Decrease `lr`, check rewards |

### NaN Values

| Symptom | Cause | Fix |
|---------|-------|-----|
| NaN loss | Gradient explosion | Reduce `lr`, `max_grad_norm` |

---

## üìö Research Background

### Theoretical Foundations

- **Global Workspace Theory** (Baars, 1988): Selective broadcasting to specialized processors
- **Information Bottleneck** (Tishby et al., 1999): Compression while preserving task-relevant info
- **TD Learning** (Sutton, 1988): Temporal difference as surprise signal

### Related Work

- **PPO** (Schulman et al., 2017): Proximal Policy Optimization
- **POPGym** (Morad et al., 2023): Partially Observable benchmarks
- **Gated RNNs** (Hochreiter & Schmidhuber, 1997; Cho et al., 2014)

### Why TD-Driven Gating?

1. **Already computed**: No additional forward passes
2. **Task-relevant**: Directly measures prediction quality
3. **Self-normalizing**: Scales with reward magnitude
4. **Theoretically grounded**: Surprise = importance

---

## üìù Citation

```bibtex
@software{gwtib2026,
  title={GWT-IB: Gated Working Memory with Information Bottleneck},
  author={Block, Dawson},
  year={2026},
  url={https://github.com/dawsonblock/GWT-IB}
}
```

---

## üìÑ License

MIT License - see LICENSE file for details.

---

## ü§ù Contributing

Contributions welcome! Please:

1. Preserve hard invariants (router equation, PPO, IB loss)
2. Add tests for new features
3. Update README for configuration changes

---

<div align="center">

**Happy Training! üöÄ**

Made with ‚ù§Ô∏è by [Dawson Block](https://github.com/dawsonblock)

</div>
